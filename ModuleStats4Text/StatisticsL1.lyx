#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}
%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}
\end_preamble
\options xcolor=svgnames
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Text Mining
\end_layout

\end_inset

Text Mining
\begin_inset Newline newline
\end_inset

Statistical Modeling of Textual Data
\begin_inset Newline newline
\end_inset

Lecture 1
\end_layout

\begin_layout Author
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Magnusson, Villani
\end_layout

\end_inset

Måns Magnusson, Mattias Villani
\end_layout

\begin_layout Institute

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\series bold
StiMa, LiU
\end_layout

\end_inset

Division of Statistics and machine learning
\begin_inset Newline newline
\end_inset

Dept.
 of Computer and Information Science
\begin_inset Newline newline
\end_inset

Linköping University 
\end_layout

\begin_layout Date
\begin_inset space \thinspace{}
\end_inset


\end_layout

\begin_layout Standard

\lang swedish
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE>>=
\end_layout

\begin_layout Plain Layout

# Paket
\end_layout

\begin_layout Plain Layout

suppressMessages(library(bayesm))
\end_layout

\begin_layout Plain Layout

suppressMessages(library(ggplot2))
\end_layout

\begin_layout Plain Layout

suppressMessages(library(grid))
\end_layout

\begin_layout Plain Layout

set.seed(42)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

library(devtools)
\end_layout

\begin_layout Plain Layout

options(scipen = 10, digits = 3)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

source("https://gist.githubusercontent.com/tonglu/9947245/raw/c5334d6dc4af472a0056
8259a9337b428f1afc80/multiplot.r")
\end_layout

\begin_layout Plain Layout

source("lecture_code.R")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Overview 'Statistics for Textual Data'
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Lecture 1
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Language models and n-grams
\end_layout

\begin_layout Itemize

\series bold
Part-of-speech tagging
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Lecture 2
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Text classification
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Lecture 3
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Topic models
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some definitions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Quote
A neutron walks into a bar and asks how much for a drink.
 The bartender replies "for you, no charge".
\end_layout

\begin_layout Itemize
Tokens
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Sentences
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Types / word types
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Document
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Corpus
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Vocabulary
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Language models - predict the next word
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename SwiftKeyWordPred.png
	scale 15

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Language models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $w_{i}$
\end_inset

 denote the 
\begin_inset Formula $i$
\end_inset

th word in a sentence.
 Let 
\begin_inset Formula $\mathbf{w}_{1}^{k}=w_{1}w_{2}\cdots w_{k}$
\end_inset

 denote a sentence of 
\begin_inset Formula $k$
\end_inset

 tokens.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The probability of a sentence 
\begin_inset Formula 
\[
p(\mathbf{w}_{1}^{n})=p(w_{1})\cdot p(w_{2}|w_{1})\cdot p(w_{3}|\mathbf{w}_{1}^{2})\cdots p(w_{n}|\mathbf{w}_{1}^{n-1})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Probability distribution over the next token in a sentence:
\begin_inset Formula 
\[
p(w_{k}|\mathbf{w}_{1}^{k-1})
\]

\end_inset


\end_layout

\begin_layout Itemize
Example:
\begin_inset Formula 
\[
p(\text{mall}|\text{I like to go to the})=0.2
\]

\end_inset


\begin_inset Formula 
\[
p(\text{school}|\text{I like to go to the})=0.001
\]

\end_inset


\end_layout

\begin_layout Itemize
Add beginning of sentence token/tag <s>.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unigram models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Unigram
\series default
 
\series bold
language models
\series default
 ignores the previous words:
\begin_inset Formula 
\[
p(w_{n}|w_{1},...,w_{n-1})=p(w_{n})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $p(w_{n})$
\end_inset

 can be estimated using 
\series bold
maximum likelihood
\series default
 
\series bold
(ML
\series default
) estimation as:
\begin_inset Formula 
\[
\hat{p}_{ML}(w_{n})=\frac{C(w_{n})}{N}
\]

\end_inset

where 
\begin_inset Formula $C(w_{n})$
\end_inset

 is the number of tokens of word type 
\begin_inset Formula $w_{n}$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Simulating a text from a bag-of-words model gives rubbish.
\end_layout

\begin_layout Quotation
Much asks into neutron asks.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Language models - n-grams
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The
\series bold
 bigram
\series default
 model
\begin_inset Formula 
\[
p(w_{n}|w_{1},...,w_{n-1})=p(w_{n}|w_{n-1})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
ML estimate:
\begin_inset Formula 
\[
\hat{p}(w_{n}|w_{n-1})=\frac{\text{Number of times word \ensuremath{w_{n}}}\text{ follows directly after \ensuremath{w_{n-1}}}}{\text{Number of times \ensuremath{w_{n-1}}appears in the text}}
\]

\end_inset


\end_layout

\begin_layout Itemize
Alternative formulation
\begin_inset Formula 
\[
\hat{p}(w_{n}|w_{n-1})=\frac{\text{\ensuremath{C(w_{n-1}},}\ensuremath{w_{n})}}{\text{\ensuremath{C(w_{n-1}}})}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The bigram language model can therefore be estimated from unigram and bigram
 counts.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Trigram model
\series default
: 
\begin_inset Formula $p(w_{n}|w_{n-1},w_{n-2})$
\end_inset

 and so on.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Evaluating language models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Unsupervised
\begin_inset Quotes erd
\end_inset

 models - no ground truth
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Evaluating language models by 
\series bold
Perplexity
\series default
 (
\series bold
PP
\series default
)
\begin_inset Formula 
\[
\mathrm{PP}=\sqrt[N]{\frac{1}{P(w_{1}w_{2}\cdots w_{N})}}=\sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_{i}|w_{1}\cdots w_{i-1})}}
\]

\end_inset


\end_layout

\begin_layout Itemize
Minimizing perplexity is the same as maximizing probability of a sentence
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Evaluate on hold-out test set
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Not necessarily correlated with human judgement
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Software n-gram models 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Python NLTK
\end_layout

\begin_layout Itemize

\family typewriter
nltk.bigrams()
\end_layout

\begin_layout Itemize

\family typewriter
nltk.trigrams()
\end_layout

\begin_layout Itemize

\family typewriter
nGramModel = nltk.NgramModel(2,text7)
\family default
 # Training a bigram model from text7
\end_layout

\begin_layout Itemize

\family typewriter
nGramModel.generate(num_words=50) 
\family default
# Simulate a text with 50 words from the model.
\end_layout

\begin_layout Standard
R
\end_layout

\begin_layout Itemize

\family typewriter
tm
\family default
 package
\end_layout

\begin_layout Itemize

\family typewriter
ngram
\family default
 package
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The sparsity problem - unigram case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Maximum likelihood estimator (MLE) for unigram model: 
\begin_inset Formula 
\[
\hat{p}_{ML}(w_{n})=\frac{C(w_{1})}{N}
\]

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of tokens in training corpus.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Problem with MLE: words not in training corpus are deemed impossible!
\begin_inset Formula 
\[
C(w_{1})=0\;\Rightarrow\;\hat{p}_{ML}(w_{n})=0
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Fixing the MLE: 
\series bold
add-one smoothing 
\series default
(
\series bold
Laplace
\series default
 
\series bold
smoothing
\series default
)
\begin_inset Formula 
\[
Pr_{Lap}(w_{1})=\frac{C(w_{1})+1}{N+V},
\]

\end_inset

where 
\begin_inset Formula $V$
\end_inset

 is the number of word types in vocabulary.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The sparsity problem - n-grams
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Bigrams
\series default
 looks for pairs of consecutive words 
\begin_inset Formula $w_{1}w_{2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The number of possible outcomes is now 
\begin_inset Formula $B=V^{2}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
n-grams can have a 
\series bold
huge outcome space
\series default
 
\begin_inset Formula $B=V^{n}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Lots of n-grams are unseen in training corpus.
 
\series bold
Sparsity
\series default
 problems!
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Add-one smoothing for n-grams
\series default

\begin_inset Formula 
\[
Pr_{Lap}(w_{1}w_{2}\cdots w_{n})=\frac{C(w_{1}w_{2}\cdots w_{n})+1}{N+B},
\]

\end_inset

where 
\begin_inset Formula $C(w_{1}w_{2}\cdots w_{n})$
\end_inset

 is the number of n-grams 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $w_{1}w_{2}\cdots w_{n}$
\end_inset

 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
in the training corpus.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood inference for multinomial data
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Data
\series default
: 
\begin_inset Formula $y=(n_{1},...n_{B})$
\end_inset

, where 
\begin_inset Formula $n_{b}$
\end_inset

 counts the number of observations in the 
\begin_inset Formula $b$
\end_inset

th category.
 
\begin_inset Formula $\sum_{j=1}^{B}n_{j}=N$
\end_inset

.
\end_layout

\begin_layout Itemize
Example: A recent survey among consumer smartphones owners in the U.S.
 showed that among the 
\begin_inset Formula $N=$
\end_inset


\begin_inset Formula $513$
\end_inset

 respondents:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $n_{1}=180$
\end_inset

 owned an iPhone
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{2}=230$
\end_inset

 owned an Android phone
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{3}=62$
\end_inset

 owned a Blackberry phone
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{4}=41$
\end_inset

 owned some other mobile phone.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\theta_{1}=Pr(\text{owns iPhone})$
\end_inset

, 
\begin_inset Formula $\theta_{2}=Pr(\text{owns Android})$
\end_inset

 etc
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Likelihood
\series default

\begin_inset Formula 
\[
p(n_{1},n_{2},...,n_{B}|\theta_{1},\theta_{2},...,\theta_{B})=const\cdot\prod_{j=1}^{B}\theta_{j}^{n_{j}}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Maximum likelihood
\series default
 (ML) estimator
\begin_inset Formula 
\[
\hat{\theta}_{b}=\frac{n_{b}}{N}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian smoothing for multinomial data
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
ML problematic when data is sparse
\series default
.
 
\begin_inset Formula $n_{b}=0$
\end_inset

 
\begin_inset Formula $\Rightarrow$
\end_inset

 
\begin_inset Formula $\hat{\theta}_{b}=0$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Smoothing
\series default
 using a 
\series bold
Bayesian prior
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Prior:
\series default
 
\begin_inset Formula $\theta\sim\mathrm{Dirichlet}(\alpha_{1},...,\alpha_{B})$
\end_inset

 with density
\begin_inset Formula 
\[
p(\theta_{1},\theta_{2},...,\theta_{B})\propto\prod_{j=1}^{B}\theta_{j}^{\alpha_{j}-1}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Distribution over the simplex
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Expected value
\series default
 and 
\series bold
variance
\series default
 of the 
\begin_inset Formula $Dirichlet(\alpha_{1},...,\alpha_{B})$
\end_inset

 distribution
\begin_inset Formula 
\begin{align*}
\mathrm{E}(\theta_{b}) & =\frac{\alpha_{b}}{\sum_{j=1}^{B}\alpha_{j}}\qquad\qquad\mathrm{V}(\theta_{b})=\frac{\mathrm{E}(\theta_{b})\left[1-\mathrm{E}(\theta_{b})\right]}{1+\sum_{j=1}^{B}\alpha_{j}}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Note that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\sum_{j=1}^{B}\alpha_{j}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is a 
\series bold
precision
\series default
 parameter.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The dirichlet distribution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
only<1>{
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang swedish
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=4>>=
\end_layout

\begin_layout Plain Layout

plt <- draw_dir_example()
\end_layout

\begin_layout Plain Layout

multiplot(plotlist = plt, cols = 2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
only<2>{
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang swedish
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=4>>=
\end_layout

\begin_layout Plain Layout

plt <- draw_dir_example()
\end_layout

\begin_layout Plain Layout

multiplot(plotlist = plt, cols = 2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
only<3>{
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang swedish
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=4>>=
\end_layout

\begin_layout Plain Layout

plt <- draw_dir_example()
\end_layout

\begin_layout Plain Layout

multiplot(plotlist = plt, cols = 2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
only<4>{
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang swedish
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=4>>=
\end_layout

\begin_layout Plain Layout

plt <- draw_dir_example()
\end_layout

\begin_layout Plain Layout

multiplot(plotlist = plt, cols = 2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bayesian smoothing for multinomial data
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Posterior
\series default
 distribution (Likelihood 
\begin_inset Formula $\times$
\end_inset

Prior) 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
Posterior:\text{ \ \ }\theta|n_{1},...,n_{B}\sim\mathrm{Dirichlet}(n_{1}+\alpha_{1},...,n_{B}+\alpha_{B})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Posterior expected value
\series default

\begin_inset Formula 
\[
E(\theta_{b}|n_{1},...,n_{B})=\frac{n_{b}+\alpha_{b}}{N+\sum_{j=1}^{B}\alpha_{j}}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Add-one (Laplace) smoothing
\series default
 obtained with uniform prior 
\begin_inset Formula $\alpha_{1}=...=\alpha_{B}=1$
\end_inset


\begin_inset Formula 
\[
E(\theta_{b}|n_{1},...,n_{B})=\frac{n_{b}+1}{N+B}
\]

\end_inset

where 
\begin_inset Formula $B=V^{n}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Not a great solution when 
\begin_inset Formula $B>>N$
\end_inset

.
 Too much probability mass on unseen words.
\end_layout

\begin_layout Itemize
Uniform prior distribution over all n-grams is stupid.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Other smoothing methods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Linear interpolation
\series default
 combines trigram, bigram and unigrams:
\begin_inset Formula 
\[
\hat{p}_{LI}(w_{n}|w_{n-1},w_{n-2})=\lambda_{1}\hat{p}(w_{n}|w_{n-1},w_{n-2})+\lambda_{2}\hat{p}(w_{n}|w_{n-1})+\lambda_{3}\hat{p}(w_{n})
\]

\end_inset


\end_layout

\begin_layout Itemize
The parameters 
\begin_inset Formula $\lambda_{1}$
\end_inset

, 
\begin_inset Formula $\lambda_{2}$
\end_inset

 and 
\begin_inset Formula $\lambda_{3}$
\end_inset

 can be chosen by cross-validation.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Katz back-off
\series default
 
\begin_inset Formula $N$
\end_inset

-gram model: use 
\begin_inset Formula $N$
\end_inset

-gram if available, otherwise back-off to 
\begin_inset Formula $N-1$
\end_inset

 gram:
\family typewriter
\size footnotesize
 
\begin_inset Formula 
\[
\hat{p}_{katz}(w_{n}|w_{n-N+1}^{n-1})=\left\{ \begin{array}{cc}
\hat{p}(w_{n}|w_{n-N+1}^{n-1}) & \text{if }C(w_{n-N+1}^{n})>0\\
\alpha(w_{n-N+1}^{n-1})\cdot\hat{p}_{katz}(w_{n}|w_{n-N+2}^{n-1}) & \text{otherwise}
\end{array}\right\} 
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
PoS-based N-grams
\series default
: use word classes to better distribute probability mass to unseen trigrams.
 Verb-Verb-Verb is not a likely sequence.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Part-of-speech tagging
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Part-of-Speech
\series default
 (
\series bold
PoS
\series default
) or 
\series bold
word classes
\series default
 - verb, noun, adjective, preposition etc:
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Examples from 45-tag Penn Treebank:
\end_layout

\begin_deeper
\begin_layout Itemize
JJ - 
\series bold
Adjective
\series default
.
 JJR - comparative.
 JJS - superlative
\end_layout

\begin_layout Itemize
NN - 
\series bold
Noun
\series default
, singular or mass, NNS - plural NNP - Proper noun, singular NNPS - Proper
 noun, plural
\end_layout

\begin_layout Itemize
VB - 
\series bold
Verb
\series default
, base form.
 VBD - past tense.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Brown corpus in NLTK: 
\series bold
The
\series default
/at 
\series bold
Fulton
\series default
/np-tl 
\series bold
County
\series default
/nn-tl 
\series bold
Grand
\series default
/jj-tl 
\series bold
Jury
\series default
/nn-tl 
\series bold
said
\series default
/vbd 
\series bold
Friday
\series default
/nr ...
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A probabilistic model for POS tagging
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
POS tagging
\series default
: determine the sequence of POS tags 
\begin_inset Formula 
\[
t_{1}^{n}=t_{1}t_{2}\cdots t_{n}
\]

\end_inset

 for the words in the sentence
\begin_inset Formula 
\[
w_{1}^{n}=w_{1}w_{2}\cdots w_{n}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Note
\series default
: each word gets a POS tag
\begin_inset Formula 
\[
\begin{array}{cccc}
w_{1} & w_{2} & \cdots & w_{n}\\
t_{1} & t_{2} & \cdots & t_{n}
\end{array}
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Aim
\series default
: posterior distribution of the tags
\begin_inset Formula 
\[
p(t_{1}^{n}|w_{1}^{n})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A probabilistic model for POS tagging, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Bayes theorem:
\begin_inset Formula 
\[
p(t_{1}^{n}|w_{1}^{n})=\frac{p(w_{1}^{n}|t_{1}^{n})p(t_{1}^{n})}{p(w_{1}^{n})}
\]

\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $p(w_{1}^{n})$
\end_inset

 does not depend on 
\begin_inset Formula $t_{1}^{n},$
\end_inset

 we can use
\begin_inset Formula 
\[
p(t_{1}^{n}|w_{1}^{n})\propto p(w_{1}^{n}|t_{1}^{n})p(t_{1}^{n})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Problem
\series default
: outcome space of 
\begin_inset Formula $t_{1}^{n}$
\end_inset

 is enormous.
 Example: 
\begin_inset Formula $n=5$
\end_inset

 with 
\begin_inset Formula $45$
\end_inset

-tag set: 
\begin_inset Formula $45^{5}=184528125$
\end_inset

.
\end_layout

\begin_layout Itemize
Example
\begin_inset Formula 
\[
\begin{array}{cccccccc}
 & \text{I} & \text{am} & \text{great} & \text{at} & \text{grammar} &  & p(t_{1}^{n}|w_{1}^{n})\\
 & t_{1} & t_{2} & t_{3} & t_{4} & t_{5} &  & 0.001\\
1 & JJ & VB & JJ & VB & VBD &  & 0.002\\
2 & VB & VB & JJ & JJ & VBD &  & 0.002\\
3 & NN & JJ & NNP & VB & JJ &  & 0.005\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots &  & \vdots\\
45^{5} & JJ & VB & DT & VB & NN &  & 0.003
\end{array}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A probabilistic model for POS tagging, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Two simplifying assumptions makes the problem manageable.
\end_layout

\begin_layout Itemize

\series bold
Assumption 1
\series default
: 
\series bold
each word depends only on its tag
\series default
: 
\begin_inset Formula 
\[
p(w_{1}^{n}|t_{1}^{n})=\prod_{i=1}^{n}p(w_{i}|t_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Assumption 2
\series default
: 
\series bold
Bigram assumption
\series default
 for the 
\series bold
tags
\series default
: 
\begin_inset Formula 
\[
p(t_{1}^{n})=\prod_{i=1}^{n}p(t_{i}|t_{i-1})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Hidden Markov model
\series default
 (
\series bold
HMM
\series default
)
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Markov model for POS tags - HMM model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename MarkovPOS.png
	scale 60

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Observation likelihoods - HMM model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename MarkovPOSWordAndTags.jpg
	lyxscale 20
	scale 12

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Part-of-speech tagging, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The POS prior
\begin_inset Formula 
\[
p(t_{1}^{n})=\prod_{i=1}^{n}p(t_{i}|t_{i-1})
\]

\end_inset

can be estimated as a bigram model from a tagged corpus.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The word distribution 
\begin_inset Formula $p(w_{i}|t_{i})$
\end_inset

 can be estimated by
\begin_inset Formula 
\[
\hat{p}(w_{i}|t_{i})=\frac{C(t_{i},w_{i})}{C(t_{i})}
\]

\end_inset


\end_layout

\begin_layout Itemize
The solution to the prediction (parsing) problem
\begin_inset Formula 
\[
\mathrm{argmax}_{t_{1}^{n}}\,p(t_{1}^{n}|w_{1}^{n})
\]

\end_inset

can be found by the 
\series bold
Viterbi
\series default
 algorithm.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Part-of-speech tagging, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Gibbs sampling can be used to draw samples from the posterior 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
p(t_{1}^{n}|w_{1}^{n})
\]

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
both using supervised data, semi-supervised and unsupervised.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can be combined with more complex probabilistic models
\end_layout

\end_deeper
\end_body
\end_document
